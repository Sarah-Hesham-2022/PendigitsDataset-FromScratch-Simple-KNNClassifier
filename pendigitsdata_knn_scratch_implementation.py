# -*- coding: utf-8 -*-
"""PendigitsData_KNN_Scratch_Implementation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Le7t6TKman_Kh-vzvib0OKcZhCk0REMv
"""

#/content/sample_data/Pendigits_train_KNN.txt
data = open("/content/sample_data/Pendigits_train_KNN.txt")
x_train = []
y_train = []
line = data.readline()
while(line):
  xTemp = []
  tempLine = line.split(" ")
  for i in tempLine:
     if(i != ''):
      xTemp.append(int(i))
  y_train.append(int(xTemp[-1]))
  del xTemp[-1]
  x_train.append(xTemp)
  line = data.readline()

print(x_train)
print(y_train)
print(len(x_train))
print(len(y_train))
print(len(x_train[0]))

#/content/sample_data/Pendigits_test_KNN.txt
data = open("/content/sample_data/Pendigits_test_KNN.txt")
x_test = []
y_test = []
line = data.readline()
while(line):
  xTemp = []
  tempLine = line.split(" ")
  for i in tempLine:
     if(i != ''):
      xTemp.append(int(i))
  y_test.append(int(xTemp[-1]))
  del xTemp[-1]
  x_test.append(xTemp)
  line = data.readline()

print(x_test)
print(y_test)
print(len(x_test))
print(len(y_test))
print(len(x_test[0]))

#Each dimension should be normalized, separately from all other dimensions.
#Specifically, for both training and test objects, each dimension should be
#transformed using function: F(v) = (v - mean) / std, using the mean and std
#of the values of that dimension on the TRAINING data.

import numpy as np
mean_train = []
sd_train = []
for i in range(len(x_train[0])):
  tempX =[]
  for j in range(len(x_train)):
    tempX.append((x_train[j][i]))
  mean_train.append(np.mean(tempX))
  sd_train.append(np.std(tempX))

print(mean_train)
print(sd_train)
print(len(mean_train))
print(len(sd_train))

for i in range(len(x_train)):
  for j in range(len(x_train[i])):
    x_train[i][j] =round((((x_train[i][j]) - mean_train[j]) / sd_train[j]),3)

print(x_train)

for i in range(len(x_test)):
  for j in range(len(x_test[i])):
    x_test[i][j] =round((((x_test[i][j]) - mean_train[j]) / sd_train[j]),3)

print(x_test)

print(len(x_train))
print(len(x_train[0]))
print(len(x_test))
print(len(x_test[0]))

final_distances = []
for i in range(len(x_test)):
  distances = []
  for j in range(len(x_train)):
    # calculating Euclidean distance
    # using linalg.norm()
    dist = round((np.linalg.norm(np.array(x_train[j]) - np.array(x_test[i]))),3)
    distances.append(dist)
  final_distances.append(distances)

print(len(final_distances))
print(len(final_distances[0]))

#If there is a tie in the class predicted by the k-nearest neighbors, then
#among the classes that have the same number of votes, the tie should
#be broken in favor of the class comes first in the Train file.

import statistics as st 

k = 1

while(k <= 9):

  y_pred = []

  for i in range(len(final_distances)):

   y_result =[] 
   indices = np.sort(np.array(final_distances[i]).argsort()[:k])

   for j in indices:
     y_result.append(y_train[j])

   y_pred.append(st.mode(y_result))

  print("When K is : " + str(k))
  print("The Predicted Class Label")
  print(y_pred)
  print("The Actual Class Label")
  print(y_test)

  correct = 0
  wrong = 0
  for i in range(len(y_test)):
    if(int(y_test[i]) == int(y_pred[i])):
      correct = correct + 1
    else:
      wrong = wrong + 1

  print("The total number of test cases is : " + str(len(y_test)))
  print("The number of correctly classified classes is : " + str(correct))
  print("The number of wrongly classified classes is : " + str(wrong))
  print("The accuracy of this case is " + str(round(correct/len(y_test)*100,3)) + "%")
  print("------------------------------------------------------------------------------------------------------")
  k = k + 1